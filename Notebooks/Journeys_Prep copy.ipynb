{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to the Notebook for Monha's and Bemi's Bachelor Project\n",
    "\n",
    "## Content\n",
    "\n",
    "In this notebook we will:\n",
    "\n",
    "1. Aggrigate our data into usable travel sequences with only the relevant data \n",
    "2. Analyse the appropriate data\n",
    "3. Create an embedding space using Word2Vec\n",
    "\n",
    "We will use the following format for the structure of the file:\n",
    "1. MD file to describe the intention of the following code followed by an explanation of the results from the code if any\n",
    "2. Code block to write code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Setup\n",
    "\n",
    "Please pip install the correct libraries for the following code to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas # Pandas for data handling\n",
    "%pip install numpy  # Maths stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data import\n",
    "\n",
    "The data used in this notebook is extracted from the Journeys table from the DB. \n",
    "\n",
    "The data in question contains ~43 mil rows. This data is all journeys traveled in the timespan of ~4 years. For the purpose of this project we wish to filter the data, such that we only work with journeys within Copenhagen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../Data/All_Journeys.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering data\n",
    "\n",
    "In order to filter our data, XXX checks need to be made to be certain a journey is within cph as well as containing information relevant for our purpose. \n",
    "\n",
    "For a journey to be within cph they need to only make use of zone 1 through 4\n",
    "1. Check if *internalStartZones* only contain zones within cph\n",
    "2. Check if *internalValidZones* only contain zones within cph\n",
    "\n",
    "For a journey to be relevant for the project, we need the fields *StartStop*, *EndStop*, *SearchStart* and *SearchEnd* to be either fully filled out or partly - that is, if Start- and EndStop are null, then SearchStart and -End need to be filled. Likewise, the fields must not match in their values; a journeys start and end should not be the same.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Copenhagen filtering\n",
    "condition_1_cph = (\n",
    "    (data['internalValidZones'].str.match(r'^(1001|1002|1003|1004)(,(1001|1002|1003|1004))*$')\n",
    "    | # or\n",
    "    pd.isna(data['internalValidZones']))\n",
    "    )\n",
    "\n",
    "condition_2_cph = (\n",
    "    (data['internalStartZones'].str.match(r'^(1001|1002|1003|1004)$'))\n",
    "    | # or\n",
    "    pd.isna(data['internalStartZones'])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cph_data_1 = data[(condition_1_cph)]\n",
    "cph_data_2 = cph_data_1[condition_2_cph]\n",
    "\n",
    "cph_data_3 = cph_data_2[ ~ (cph_data_2['SearchStart'].str.contains(\"okation\", na=False)\n",
    "                                             | #Or\n",
    "                                             cph_data_2['SearchStart'].str.contains(\"zoner\", na=False))]\n",
    "cph_data_4 = cph_data_3[( ~ (cph_data_3['SearchEnd'].str.contains(\"zoner\", na=False) \n",
    "                                            | #Or\n",
    "                                            cph_data_3['SearchEnd'].str.contains(\"okation\", na=False)))]\n",
    "\n",
    "# next two filters are English filters of the first\n",
    "cph_data_5 = cph_data_4[( ~ (cph_data_4['SearchEnd'].str.contains(\"zones\", na=False) \n",
    "                                            | #Or\n",
    "                                            cph_data_4['SearchEnd'].str.contains(\"ocation\", na=False)))]\n",
    "\n",
    "cph_data_6 = cph_data_5[( ~ (cph_data_5['SearchStart'].str.contains(\"zones\", na=False) \n",
    "                                            | #Or\n",
    "                                            cph_data_5['SearchStart'].str.contains(\"ocation\", na=False)))]\n",
    "\n",
    "# Next filter is to remove entries where one of the matching search-x or x-stop are Null\n",
    "cph_data_7 = cph_data_6[(\n",
    "                                        ( ~ (pd.isna(cph_data_6['SearchStart'])) & ~ (pd.isna(cph_data_6['SearchEnd'])))\n",
    "                                        | # Or\n",
    "                                        ( ~ (pd.isna(cph_data_6['StartStop'])) & ~ (pd.isna(cph_data_6['EndStop'])))\n",
    "                                        )]\n",
    "\n",
    "# Next filter removes all entries where SearchStart and SearchEnd contain the same value\n",
    "cph_data = cph_data_7[(\n",
    "                        ~(cph_data_7['SearchStart'] == cph_data_7['SearchEnd'])\n",
    "                        )]\n",
    "\n",
    "cph_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cph_data = data[(condition_1_cph)]\n",
    "cph_data = cph_data[condition_2_cph]\n",
    "\n",
    "cph_data = cph_data[ ~ (cph_data['SearchStart'].str.contains(\"okation\", na=False)\n",
    "                                             | #Or\n",
    "                                             cph_data['SearchStart'].str.contains(\"zoner\", na=False))]\n",
    "cph_data = cph_data[( ~ (cph_data['SearchEnd'].str.contains(\"zoner\", na=False) \n",
    "                                            | #Or\n",
    "                                            cph_data['SearchEnd'].str.contains(\"okation\", na=False)))]\n",
    "\n",
    "# next two filters are English filters of the first\n",
    "cph_data = cph_data[( ~ (cph_data['SearchEnd'].str.contains(\"zones\", na=False) \n",
    "                                            | #Or\n",
    "                                            cph_data['SearchEnd'].str.contains(\"ocation\", na=False)))]\n",
    "\n",
    "cph_data = cph_data[( ~ (cph_data['SearchStart'].str.contains(\"zones\", na=False) \n",
    "                                            | #Or\n",
    "                                            cph_data['SearchStart'].str.contains(\"ocation\", na=False)))]\n",
    "\n",
    "# Next filter is to remove entries where one of the matching search-x or x-stop are Null\n",
    "cph_data = cph_data[(\n",
    "                                        ( ~ (pd.isna(cph_data['SearchStart'])) & ~ (pd.isna(cph_data['SearchEnd'])))\n",
    "                                        | # Or\n",
    "                                        ( ~ (pd.isna(cph_data['StartStop'])) & ~ (pd.isna(cph_data['EndStop'])))\n",
    "                                        )]\n",
    "\n",
    "# Next filter removes all entries where SearchStart and SearchEnd contain the same value\n",
    "cph_data = cph_data[(\n",
    "                        ~(cph_data['SearchStart'] == cph_data['SearchEnd'])\n",
    "                        )]\n",
    "\n",
    "cph_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing to see whether our filtering worked\n",
    "\n",
    "Since we are handling a very large amount of data, it can be difficult to scim through the data in order to see if it is as intended. These tests are used in order to detect whether or not rows that are not supposed to be in our data is in our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 1 for whether our data contain seachEnd with contains 'lokation' or 'location'\n",
    "lokation_count = cph_data[cph_data['SearchEnd'].str.contains(\"okation\", na=False)].count()\n",
    "print(f\"Amount of 'Lokation' entires in 'SearchEnd' : {lokation_count['SearchEnd']}\")\n",
    "\n",
    "location_count = cph_data[cph_data['SearchEnd'].str.contains(\"ocation\", na=False)].count()\n",
    "print(f\"Amount of 'Location' entires in 'SearchEnd' : {lokation_count['SearchEnd']}\")\n",
    "\n",
    "# Test 2 for whether our data contain seachStart with contains 'lokation' or 'location'\n",
    "lokation_count_s = cph_data[cph_data['SearchStart'].str.contains(\"okation\", na=False)].count()\n",
    "print(f\"Amount of 'Lokation' entires in 'SearchStart' : {lokation_count_s['SearchStart']}\")\n",
    "\n",
    "location_count_s = cph_data[cph_data['SearchStart'].str.contains(\"ocation\", na=False)].count()\n",
    "print(f\"Amount of 'Location' entires in 'SearchStart' : {location_count_s['SearchStart']}\")\n",
    "\n",
    "# Test 3 for whether our data contain SearchStart with 'zones' or 'zoner'\n",
    "zones_count = cph_data[cph_data['SearchEnd'].str.contains(\"zones\", na=False)].count()\n",
    "print(f\"Amount of 'zones' entires in 'SearchEnd' : {zones_count['SearchEnd']}\")\n",
    "\n",
    "zones_count_r = cph_data[cph_data['SearchEnd'].str.contains(\"zoner\", na=False)].count()\n",
    "print(f\"Amount of 'zoner' entires in 'SearchEnd' : {zones_count_r['SearchEnd']}\")\n",
    "\n",
    "# Test 4 for whether our data contain None in 3 or more fields (startStop, EndStop, SearchStart and SearchEnd)\n",
    "num_nulls = cph_data[['StartStop', 'EndStop', 'SearchStart', 'SearchEnd']].isna().sum(axis=1)\n",
    "b = (num_nulls >= 3).any()\n",
    "print(f\"Does the data contain a row which 3 of StartStop, EndStop, SearchStart or SearchEnd is null: {b}\")\n",
    "\n",
    "# Test 5 for whether our data contain duplicates in matching fields, i.e. StartStop == EndStop\n",
    "duplicates_in_stop = cph_data[(cph_data['StartStop'] == cph_data['EndStop'])].count()\n",
    "print(f\"Amount of matching values in StartStop and EndStop : {duplicates_in_stop['StartStop']}\")\n",
    "\n",
    "\n",
    "# Test 6 for whether our data contain duplicates in matching fields, i.e. SearchStart == SearchEnd\n",
    "duplicates_in_stop = cph_data[(cph_data['SearchStart'] == cph_data['SearchEnd'])].count()\n",
    "print(f\"Amount of matching values in SearchStart and SearchEnd : {duplicates_in_stop['SearchStart']}\")\n",
    "\n",
    "# Test 7 for whether our data contain three of the fields filled.\n",
    "num_filled = ~(cph_data[['StartStop', 'EndStop', 'SearchStart', 'SearchEnd']].isna()).sum(axis=1)\n",
    "b = (num_filled == 3).any()\n",
    "print(f\"Does the data contain a row which 3 of StartStop, EndStop, SearchStart or SearchEnd are filled: {b}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From ID to Station\n",
    "\n",
    "We wish to get rid of the IDs used in StartStop and EndStop as these do not really give us a direct understanding of what station is used in a journey. Therefore we will use the table SJWaypoints to match a given Station-Id with a 'Name'. We then wish replace all the entries in our cph_data such that we do not have these integers as IDs but rather a stop-name. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_name_data = pd.read_csv('../Data/SJ_results.csv')\n",
    "grouped_id_name = id_to_name_data[['Id', 'Name']].groupby('Id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_id_name.value_counts()\n",
    "id_name_list = grouped_id_name.agg(list)\n",
    "\n",
    "id_to_name_dict = {}\n",
    "\n",
    "for id, frame in grouped_id_name:\n",
    "    if id not in id_to_name_dict:\n",
    "        id_to_name_dict[id] = frame['Name'].iloc[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change of cph_data\n",
    "\n",
    "We now wish to replace all Ids in cph_data from StartStop and EndStop with the associated Name from the dict. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = cph_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def id_to_station(row):\n",
    "    if pd.notna(row['StartStop']) : \n",
    "        row['StartStop'] = id_to_name_dict[row['StartStop']]\n",
    "        row['EndStop'] = id_to_name_dict[row['EndStop']]\n",
    "    return row\n",
    "\n",
    "test_df = test_df.apply(id_to_station, axis=1)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequences\n",
    "\n",
    "We now wish to make sequences from the journeys. The sequnces should either be a value pair of SearchStart and Searchend or a pair of StartStop and EndStop. To do this we simply collect the pairs from the dataframe where StartStop and EndStop Id's are \"translated\" to station names. \n",
    "\n",
    "When making the sequences, certain questions arrise about the data. For instance, of the 3,4 mil datapoints, only 64 of the datapoints contain a value *only* in StartStop and EndStop. (```python test_df[~(pd.isna(test_df['StartStop'])) & (pd.isna(test_df['SearchStart']))]```)\n",
    "\n",
    "Another important decision is deciding on how to extract stations from SearchStart and SearchEnd, since a lot of the entries does not consist of a directly matching station. i.e. 'Hovedebanegården' being the SearchStart for the station 'København H'. Thus we need to match these inconsistent strings with a consistent naming convention. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "pattern = r' (\\d\\d)'\n",
    "\n",
    "sequences = []\n",
    "\n",
    "def get_sequence(row) -> None:\n",
    "    if (pd.isna(row['StartStop'])):\n",
    "        start   = re.sub(pattern, \"\", row['SearchStart'])\n",
    "        end     = re.sub(pattern, \"\", row['SearchEnd'])\n",
    "        sequences.append([start, end])\n",
    "    else:\n",
    "        start   = re.sub(pattern, \"\", row['StartStop'])\n",
    "        end     = re.sub(pattern, \"\", row['EndStop'])\n",
    "        sequences.append([start, end])\n",
    "\n",
    "test_df.apply(get_sequence, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
